{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2imkixTjwzwK"
   },
   "source": [
    "This notebook contains the code to use BERT with an extra layer of nodes added at the output to predict wether a movie review is positve or negative. Hopefully it will show the improved performance due to transfer learning at the cost of increased training time and model size compared to the models in the other notebook in this repo.\n",
    "\n",
    "but I hope to demonstrate some of the pros and cons of using BERT compared to simpler models and vectorisation techniques.\n",
    "I have also commented the code myself to demonstrate understanding of what is required to fine tune BERT to a specific task.\n",
    "\n",
    "Training takes approx. 1 hour with GPU on google colab.\n",
    "Model file is too large to send over via email unfortunately so you can't skip the training unfortunately\n",
    "\n",
    "## MAKE SURE YOU ENABLE GPU FOR GOOGLE COLAB!!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V1z48se9wzwR",
    "outputId": "6e08acba-3720-4d40-d4a0-8f45bdd23872"
   },
   "outputs": [],
   "source": [
    "!pip install bert-for-tf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RdwLOw9uwzwS",
    "outputId": "d86e4555-cee5-4aac-f7fa-14001afe94fa"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Import modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import bert\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import  Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import re\n",
    "\n",
    "print(\"TensorFlow Version:\",tf.__version__)\n",
    "print(\"Hub version: \",hub.__version__)\n",
    "pd.set_option('display.max_colwidth',1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mgjstFv3wzwT"
   },
   "source": [
    "## Load Data\n",
    "Load dataset using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6eN95Do6xuMl"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load all files from a directory in a DataFrame.\n",
    "def load_directory_data(directory):\n",
    "  data = {}\n",
    "  data[\"sentence\"] = []\n",
    "  data[\"sentiment\"] = []\n",
    "  for file_path in os.listdir(directory):\n",
    "    with tf.io.gfile.GFile(os.path.join(directory, file_path), \"r\") as f:\n",
    "      data[\"sentence\"].append(f.read())\n",
    "      data[\"sentiment\"].append(re.match(\"\\d+_(\\d+)\\.txt\", file_path).group(1))\n",
    "  return pd.DataFrame.from_dict(data)\n",
    "\n",
    "# Merge positive and negative examples, add a polarity column and shuffle.\n",
    "def load_dataset(directory):\n",
    "  pos_df = load_directory_data(os.path.join(directory, \"pos\"))\n",
    "  neg_df = load_directory_data(os.path.join(directory, \"neg\"))\n",
    "  pos_df[\"polarity\"] = 1\n",
    "  neg_df[\"polarity\"] = 0\n",
    "  # return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)\n",
    "  return pd.concat([pos_df, neg_df])\n",
    "\n",
    "# Download and process the dataset files.\n",
    "def download_and_load_datasets(force_download=False):\n",
    "  dataset = tf.keras.utils.get_file(\n",
    "      fname=\"aclImdb.tar.gz\", \n",
    "      origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", \n",
    "      extract=True)\n",
    "  \n",
    "  train_df = load_dataset(os.path.join(os.path.dirname(dataset), \n",
    "                                       \"aclImdb\", \"train\"))\n",
    "  test_df = load_dataset(os.path.join(os.path.dirname(dataset), \n",
    "                                      \"aclImdb\", \"test\"))\n",
    "  \n",
    "  return train_df.drop(columns = ['sentiment']), test_df.drop( columns = ['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AeEsSE2Sx7Pq",
    "outputId": "7bce863e-aad9-4a39-fc67-dc65486b053b"
   },
   "outputs": [],
   "source": [
    "train, test = download_and_load_datasets()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "id": "RmOOGmfrzBUH",
    "outputId": "18db0fc7-7e1c-4d34-b07f-8fc5cf34f1c8"
   },
   "outputs": [],
   "source": [
    "train.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "llN0J44_wzwV"
   },
   "source": [
    "## Preprocessing\n",
    "BERT creates embedding from 3 inputs: token, segment & position embeddings. Here we will create functions to create these inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lR_UT6IdwzwW"
   },
   "outputs": [],
   "source": [
    "# Functions for constructing BERT Embeddings: input_ids, input_masks, input_segments and Inputs\n",
    "\n",
    "## this function creates the mask embeddings where simply 1 for real tokens and 0 for embeddings\n",
    "MAX_SEQ_LEN=500 # max sequence length\n",
    "def get_masks(tokens):\n",
    "    \"\"\"Masks: 1 for real tokens and 0 for paddings\"\"\"\n",
    "    return [1]*len(tokens) + [0] * (MAX_SEQ_LEN - len(tokens))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qos2KqbBwzwW"
   },
   "outputs": [],
   "source": [
    "\"\"\" this function creates the segment embeddings i.e. BERT is trained on 2 sentences to predict masked words and\n",
    "    the next sentence therefore the input shpuld be 2 sentences. With 0 for the first and 1 for the second\"\"\"\n",
    "\n",
    "def get_segments(tokens):\n",
    "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"  \n",
    "    segments = []\n",
    "    current_segment_id = 0\n",
    "    for token in tokens:\n",
    "        segments.append(current_segment_id)\n",
    "        if token == \"[SEP]\":\n",
    "            current_segment_id = 1\n",
    "    return segments + [0] * (MAX_SEQ_LEN - len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5y5KJv0YwzwW"
   },
   "outputs": [],
   "source": [
    "## gets token ids from BERT's vocabulary\n",
    "def get_ids(tokens, tokenizer):\n",
    "    \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens,)\n",
    "    input_ids = token_ids + [0] * (MAX_SEQ_LEN - len(token_ids))\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZHWhf5ZdwzwX"
   },
   "outputs": [],
   "source": [
    "## tokenize the input, cut it to the max length, and then create input, mask and segment embeddings\n",
    "def create_single_input(sentence, tokenizer, max_len):\n",
    "    \"\"\"Create an input from a sentence\"\"\"\n",
    "    stokens = tokenizer.tokenize(sentence)\n",
    "    stokens = stokens[:max_len]\n",
    "    stokens = [\"[CLS]\"] + stokens + [\"[SEP]\"]\n",
    " \n",
    "    ids = get_ids(stokens, tokenizer)\n",
    "    masks = get_masks(stokens)\n",
    "    segments = get_segments(stokens)\n",
    "    \n",
    "    return ids, masks, segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EsQzbw2vwzwX"
   },
   "outputs": [],
   "source": [
    " ## create features out of whole movie review, NOT JUST FIRST 2 SENTENCES!!\n",
    "def convert_sentences_to_features(sentences, tokenizer):\n",
    "    \"\"\"Convert sentences to features: input_ids, input_masks and input_segments\"\"\"\n",
    "    input_ids, input_masks, input_segments = [], [], []\n",
    " \n",
    "    for sentence in tqdm(sentences,position=0, leave=True):\n",
    "        ids,masks,segments=create_single_input(sentence,tokenizer,MAX_SEQ_LEN-2)\n",
    "        assert len(ids) == MAX_SEQ_LEN\n",
    "        assert len(masks) == MAX_SEQ_LEN\n",
    "        assert len(segments) == MAX_SEQ_LEN\n",
    "        input_ids.append(ids)\n",
    "        input_masks.append(masks)\n",
    "        input_segments.append(segments)\n",
    "\n",
    "    return [np.asarray(input_ids, dtype=np.int32), \n",
    "          np.asarray(input_masks, dtype=np.int32), \n",
    "          np.asarray(input_segments, dtype=np.int32)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZiVd2tbDwzwX"
   },
   "outputs": [],
   "source": [
    "## use bert tokenizer by loading bert vocabualry and tokenizer\n",
    "def create_tonkenizer(bert_layer):\n",
    "    \"\"\"Instantiate Tokenizer with vocab\"\"\"\n",
    "    vocab_file=bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "    do_lower_case=bert_layer.resolved_object.do_lower_case.numpy() \n",
    "    tokenizer=bert.bert_tokenization.FullTokenizer(vocab_file,do_lower_case)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PDDQkEc22_i4"
   },
   "source": [
    "## create instance of bert model\n",
    "- add 768 nodes with relu's and 2 output nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0FTddDFnwzwY",
    "outputId": "72a45c0d-c672-482c-9e85-f7e42b8afbea"
   },
   "outputs": [],
   "source": [
    "def nlp_model(callable_object):\n",
    "    # Load the pre-trained BERT base model\n",
    "    bert_layer = hub.KerasLayer(handle=callable_object, trainable=True)  \n",
    "   \n",
    "    # BERT layer three inputs: ids, masks and segments\n",
    "    input_ids = Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32, name=\"input_ids\")           \n",
    "    input_masks = Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32, name=\"input_masks\")       \n",
    "    input_segments = Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32, name=\"segment_ids\")\n",
    "    \n",
    "    inputs = [input_ids, input_masks, input_segments] # BERT inputs\n",
    "    pooled_output, sequence_output = bert_layer(inputs) # BERT outputs\n",
    "    \n",
    "    # Add a hidden layer\n",
    "    x = Dense(units=768, activation='relu')(pooled_output)\n",
    "    x = Dropout(0.1)(x)\n",
    " \n",
    "    # Add output layer\n",
    "    outputs = Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "    # Construct a new model\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "model = nlp_model(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2kl2hkoX4A1h"
   },
   "source": [
    "Lets check we are creating features correctly for reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7kp4BV7q3ww0",
    "outputId": "d347fa0d-49f1-4526-a10e-113c4b7acd88"
   },
   "outputs": [],
   "source": [
    "review = train['sentence'].head(1)\n",
    "tokenizer = create_tonkenizer(model.layers[3])\n",
    "features = convert_sentences_to_features(review, tokenizer)\n",
    "                    \n",
    "print(review)\n",
    "print('token_ids : ',features[0])\n",
    "print('mask embeddings : ', features[1])\n",
    "print('segment ids : ', features[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aJYi5_MZ9Z5i"
   },
   "source": [
    "## Model Training\n",
    " Looks fine lets train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_NNTTXRY9hgq",
    "outputId": "9f7e6448-4fe9-4b36-d9d0-5b38613909ba"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "train = train.sample(frac=1) # Shuffle the dataset\n",
    "train_frac = int(0.75*train.shape[0])\n",
    "train_df = train[:train_frac]\n",
    "val_df = train[train_frac:]\n",
    "\n",
    "tokenizer = create_tonkenizer(model.layers[3])\n",
    "X_train = convert_sentences_to_features(train_df['sentence'], tokenizer)\n",
    "X_val = convert_sentences_to_features(val_df['sentence'], tokenizer)\n",
    "X_test = convert_sentences_to_features(test['sentence'], tokenizer)\n",
    "\n",
    "\n",
    "\n",
    "y_train = to_categorical(train_df['polarity'].values)\n",
    "y_val = to_categorical(val_df['polarity'].values)\n",
    "y_test = to_categorical(test['polarity'].values)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VRLxQljq_DWk",
    "outputId": "8c52ea55-6e2c-4d22-a17d-f2798e27173d"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 2\n",
    "\n",
    "# Use Adam optimizer to minimize the categorical_crossentropy loss\n",
    "opt = Adam(learning_rate=2e-5)\n",
    "model.compile(optimizer=opt, \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Fit the data to the model\n",
    "history = model.fit(X_train, y_train,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    epochs=EPOCHS,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    verbose = 1)\n",
    "\n",
    "# Save the trained model\n",
    "model.save('nlp_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qcj0qmLVTV4D"
   },
   "source": [
    "## Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e7bZsZfFC7Xn"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the pretrained nlp_model\n",
    "from tensorflow.keras.models import load_model\n",
    "new_model = load_model('nlp_model.h5',custom_objects={'KerasLayer':hub.KerasLayer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wIg0go-ES5dg"
   },
   "outputs": [],
   "source": [
    "# Predict on test dataset\n",
    "from sklearn.metrics import classification_report\n",
    "pred_test = np.argmax(new_model.predict(X_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nLOdI-uATKrX",
    "outputId": "1cfa0e9a-1df5-4bdc-962b-5746dc102168"
   },
   "outputs": [],
   "source": [
    "print(classification_report(np.argmax(y_test,axis=1), pred_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xo9ngl9bTTLa"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "sentiment_analysis_BERT.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
