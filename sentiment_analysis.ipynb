{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary, aims & improvements\n",
    "\n",
    "This notebook contains the code to load the classify reviews from the imdb review dataset into positve and negative reviews. It contains instructions to download the code and preprocess the reviews from the command line. I have then created classes for 3 core parts of the process: loading and cleaning the data, vectorising the data and training a model to predict the sentiment of a movie review. I have also created a master class which inherits from these 3 classes so that the model can make a prediction on a review from the raw text data with just one function call from the master class. \n",
    "\n",
    "Workflow:\n",
    "- Instructions to download dataset and some minor preprocessing\n",
    "- Basic data cleaning on reviews, with print statements to notify user if dataset hasn't been downloaded or extracted\n",
    "- Vectorisation, defualt tf-idf method but user can easily change the method\n",
    "- Model training, I trained 2 classifiers a logistic regression model as it is a simple model that provides interpretability to to validate the training procedure. I also trained a svm model as they tend to perform well on sparse datasets. Tuning of hyperparameters was done on the fly with validation set.\n",
    "- Testing: recorded the test accuracy of both models: Note I also implemented a classifier fine tuned from BERT in a seperate notebook\n",
    "- Productionise: Create class that can combine the data cleaning, vectorisation and model to make predictions on raw text data. I have saved each subclass and the parent class and reloaded them to demonstrate how this could be used in production\n",
    "- Tests: I ahve completed some minor end to end tests e.g. empty reviews and extremely long reviews\n",
    "\n",
    "Results (Accuracy):\n",
    "- Logistic Regression : 0.88024\n",
    "- SVM : 0.88324\n",
    "- BERT : 0.92\n",
    "\n",
    "Positives about this system:\n",
    "- The vectorisation class can be easily provide different vectorisation methods by changing one argument when it is called e.g. 'bag of words'.\n",
    "- Code is modular and I have demonstrated how each instance of each class can be saved and loaded to create the master class. This is also done for the master class to demonstrate how this could easily be put into production\n",
    "- I have an easily callable function for end to end predictions of reviews from raw text data, suitable for production needs\n",
    "\n",
    "Production:\n",
    "I would simply load an instance of the master class that is created after training. This can be loaded for production to male predictions on raw text data. An API would be created to handle request for predictions and also how to load the data from another source e.g. csv to then pass to my model. Also use software like docker to manage versions of dpendencies.\n",
    "\n",
    "If labelled data is available while inn production I would monitor the accuracy of the system to check it is not changing form the accuracy achieved on the test set, Also precision and recall scores. If labelled data is unavailable I would monitor the proportion of positive predictions compared to negative, as this could potentially signify a change in the distribution of the labels from the dataset to the data being used in production.\n",
    "\n",
    "Analysis & Improvements:\n",
    "- SVM only produces a very minor accuracy boost compared to logistic regression at the cost of lost interpretability. With more time I would explore more kernels for the SVM to try and boost the accuracy\n",
    "- Data cleaning is very basic. I would like to experiment with stemming and n-grams to try and improve results\n",
    "- BERT achieves superior results with about an hours training on google colab with GPU(seperate notebook)--- You can skip training and load the model I trained form the repo. Leverages transfer learning would likely also perform better on OOD inputs as a result of knowledge transfer from large corpus. I have not done any productionalisation of this model though and would like to do this with more time.\n",
    "- only a few end-to-end tests are implemented. I would like to try more and also test each class individually rather than the whole system\n",
    "- Would like to automate download and extraction of dataset but not have to do it from the command line."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Before starting make sure you have downloaded the dataset from : http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "\n",
    "Then extract the data by running the following commands from the command line:\n",
    "1) tar -xf aclImdb_v1.tar.gz\n",
    "2) cd aclImdb && mkdir data\n",
    "3) for split in train test; do for sentiment in pos neg; do for file in $split/$sentiment/*; do cat $file >> data/full_${split}.txt; echo >> data/full_${split}.txt; done; done; done;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & Clean Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self):\n",
    "        self.data_path = 'aclImdb/data/'\n",
    "        if not os.path.isfile('aclImdb_v1.tar.gz'):\n",
    "            print (\"Haven't downloaded dataset\")\n",
    "            \n",
    "        if not os.path.isdir(self.data_path):\n",
    "            print(\"haven't extracted data\")\n",
    "            \n",
    "    def read_data(self):\n",
    "        data_path = self.data_path\n",
    "        reviews_train = []\n",
    "        reviews_test = []\n",
    "     \n",
    "        for line in open(data_path + 'full_train.txt', 'r', encoding=\"utf8\"):\n",
    "            reviews_train.append(line.strip())\n",
    "\n",
    "        for line in open(data_path + 'full_test.txt', 'r', encoding=\"utf8\"):\n",
    "            reviews_test.append(line.strip())\n",
    "\n",
    "        return reviews_train, reviews_test\n",
    "    \n",
    "    def clean_data(self, reviews):\n",
    "        ##remove these characters\n",
    "        REPLACE_NO_SPACE = re.compile(\"[.;:!\\'?,\\\"()\\[\\]]\")\n",
    "        ##replace these characters with space\n",
    "        REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "        \n",
    "        reviews = [REPLACE_NO_SPACE.sub(\"\", line.lower()) for line in reviews]\n",
    "        reviews = [REPLACE_WITH_SPACE.sub(\" \", line) for line in reviews]\n",
    "        \n",
    "        return reviews\n",
    "    \n",
    "    def read_and_clean(self):\n",
    "        reviews_train, reviews_test = self.read_data()\n",
    "        reviews_train = self.clean_data(reviews_train)\n",
    "        reviews_test = self.clean_data(reviews_test)\n",
    "        \n",
    "        return reviews_train, reviews_test\n",
    "        \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader()\n",
    "reviews_train, reviews_test = loader.read_and_clean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorise Reviews\n",
    "selct one vectorisation method from 'one hot encode', 'bag of words' and 'tf-idf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectoriser():    \n",
    "    def __init__(self, mode = 'tf-idf'):\n",
    "        \n",
    "        if mode == 'tf-idf':\n",
    "            vectorizer = TfidfVectorizer()\n",
    "        \n",
    "        elif mode == 'one hot encode':\n",
    "            vectorizer = CountVectorizer(binary=True)\n",
    "        \n",
    "        elif mode == 'bag of words':\n",
    "            vectorizer = CountVectorizer(binary=False)\n",
    "            \n",
    "        else:\n",
    "            print('error! specify mode as one of the following : ')\n",
    "            print(\"'tf-idf', 'one hot encode', 'bag of words'\")\n",
    "            return\n",
    "        \n",
    "        self.vectorizer = vectorizer\n",
    "        \n",
    "    def fit(self, reviews_train, reviews_test):\n",
    "        vectorizer = self.vectorizer        \n",
    "        vectorizer.fit(reviews_train)\n",
    "        X = vectorizer.transform(reviews_train)\n",
    "        X_test = vectorizer.transform(reviews_test)\n",
    "        \n",
    "        self.vec_method = vectorizer\n",
    "        \n",
    "        return X, X_test\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## I have picked tf-idf to vectorise the data\n",
    "vectoriser = Vectoriser(mode = 'tf-idf')\n",
    "X, X_test = vectoriser.fit(reviews_train, reviews_test)\n",
    "\n",
    "## both test and train targets have equal number of reviews and are evenly split between positve and negative reviews as well as being ordered\n",
    "\n",
    "target = [1 if i < 12500 else 0 for i in range(25000)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Classifier\n",
    "\n",
    "Here we will train a logistic regression classifier to give an interpretable model and a baseline model. We will also train an svm to try and improve upon that without much more training time. We will then try and leverage transfer learning by deploying BERT to further increase the accuracy at the cost of increased training time and model size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, target, train_size = 0.75\n",
    ")\n",
    "\n",
    "## tune the regularisation parameter on validation set\n",
    "best_acc = 0\n",
    "c_opt = 0\n",
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
    "    \n",
    "    lr = LogisticRegression(C=c)\n",
    "    lr.fit(X_train, y_train)\n",
    "    \n",
    "    acc = accuracy_score(y_val, lr.predict(X_val))\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, acc))\n",
    "    \n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        c_opt = c\n",
    "    \n",
    "\n",
    "    \n",
    "log_reg_model = LogisticRegression(C=c_opt)\n",
    "log_reg_model.fit(X_train, y_train)\n",
    "\n",
    "acc = accuracy_score(target, log_reg_model.predict(X_test))\n",
    "print('')\n",
    "print (\"Test Accuracy for C=%s: %s\" \n",
    "       % (c_opt, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_to_coef = {\n",
    "    word: coef for word, coef in zip(\n",
    "        vectoriser.vec_method.get_feature_names(), log_reg_model.coef_[0]\n",
    "    )\n",
    "}\n",
    "print('words likely to lead to positive review')\n",
    "for best_positive in sorted(\n",
    "    feature_to_coef.items(), \n",
    "    key=lambda x: x[1], \n",
    "    reverse=True)[:5]:\n",
    "    print (best_positive)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('words likely to lead to negative review')\n",
    "for best_negative in sorted(\n",
    "    feature_to_coef.items(), \n",
    "    key=lambda x: x[1])[:5]:\n",
    "    print (best_negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM\n",
    "Small boost in accuracy for loss in interpretability "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = 0\n",
    "c_opt = 0\n",
    "\n",
    "\n",
    "for c in [0.01, 0.05, 0.25, 0.5, 1,1.5,2]:\n",
    "    \n",
    "    svm = LinearSVC(C=c)\n",
    "    svm.fit(X_train, y_train)\n",
    "    acc = accuracy_score(y_val, svm.predict(X_val))\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, acc))\n",
    "    \n",
    "    if acc > best_acc:\n",
    "        c_opt = c\n",
    "        best_acc = acc\n",
    "        \n",
    "        \n",
    "svm_mod = LinearSVC(C=c_opt)\n",
    "svm_mod.fit(X_train, y_train)\n",
    "\n",
    "acc = accuracy_score(target, svm_mod.predict(X_test))\n",
    "print('')\n",
    "print (\"Test Accuracy for C=%s: %s\" \n",
    "       % (c_opt, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Production and testing \n",
    "\n",
    "create instance of final model by passing instances of selected trained model and associated Vectoriser() and DataLoader()\n",
    "Logic protects against very long querys and empty querys. Both querys in string and list of querys accepted.\n",
    "\n",
    "Save model from training using joblib modules and instances of data loader and vectoriser using pickle module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib \n",
    "import pickle\n",
    "\n",
    "joblib.dump(svm_mod, 'saved_model.joblib') \n",
    "# vectoriser, loader\n",
    "with open('vectoriser.pickle', 'wb') as pickle_file:\n",
    "    pickle.dump(vectoriser, pickle_file)\n",
    "    \n",
    "with open('loader.pickle', 'wb') as pickle_file:\n",
    "    pickle.dump(loader, pickle_file)\n",
    "\n",
    "# pickle.dump(loader, 'loader.pickle')\n",
    "\n",
    "with open('loader.pickle', 'rb') as f:\n",
    "    prod_loader = pickle.load(f)\n",
    "    \n",
    "with open('vectoriser.pickle', 'rb') as f:\n",
    "    prod_vec = pickle.load(f)\n",
    "\n",
    "\n",
    "prod_model = joblib.load('saved_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinalModel():\n",
    "    def __init__(self, model, vectoriser, data_loader, max_len):\n",
    "        self.model = model\n",
    "        self.vectoriser = vectoriser\n",
    "        self.data_loader = data_loader\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def valid_query(self,query):\n",
    "        valid = True\n",
    "        if (len(query) == 0) or (query == None):\n",
    "            valid = False\n",
    "            print('empty query!')  \n",
    "            \n",
    "        if len(query) > max_len:\n",
    "            print('query too long!')\n",
    "            valid = False\n",
    "        return valid\n",
    "        \n",
    "        \n",
    "    def predict(self, query):\n",
    "        data_loader = self.data_loader\n",
    "        vectoriser = self.vectoriser\n",
    "        model = self.model\n",
    "        \n",
    "        if not isinstance(query,list):\n",
    "            query = [query]\n",
    "        clean_query = data_loader.clean_data(query)\n",
    "        \n",
    "        valid = [self.valid_query(single_query) for single_query in clean_query]\n",
    "        if not all(valid):\n",
    "            print('Invalid query(s)!')\n",
    "            \n",
    "            valid_idxs = np.array((valid))\n",
    "            idxs = []\n",
    "            bad_querys = []\n",
    "            for i, bl in enumerate(valid):\n",
    "                if not bl:\n",
    "                    idxs.append(i)\n",
    "                    bad_querys.append(query[i])\n",
    "                    \n",
    "            print('bad_indexs : ', idxs)\n",
    "            print('bad querys : ', bad_querys)\n",
    "            return None\n",
    "        \n",
    "        X = self.vectoriser.vec_method.transform(clean_query)\n",
    "        pred = self.model.predict(X)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the maximum accpetable length of a review as little longer than the longest in the training data\n",
    "max_len = 0\n",
    "for review in reviews_train:\n",
    "    l = len(review)\n",
    "    if l> max_len:\n",
    "        max_len = l\n",
    "        \n",
    "max_len*=1.1\n",
    "\n",
    "\n",
    "## test end to end querys from raw text on final model from test data and check accuracy is the same\n",
    "\n",
    "query = reviews_test.copy()\n",
    "\n",
    "production_model = FinalModel(prod_model, prod_vec, prod_loader, max_len)\n",
    "\n",
    "predictions = production_model.predict(query)\n",
    "\n",
    "acc = accuracy_score(target, predictions)\n",
    "print('')\n",
    "print (\"Test Accuracy for final model : %s\" \n",
    "       % ( acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have created a class that can make predictions from unprocessed text data we can save it as a complte model and reload to deomonstrate how we could use this in production.\n",
    "\n",
    "Here I also implement some tests. Note currently the model provides no predictions if a single query is deemed invalid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save and load final model to demonstrate how this could be used in production and then test it against new querys\n",
    "with open('production_model.pickle', 'wb') as pickle_file:\n",
    "    pickle.dump(production_model, pickle_file)\n",
    "\n",
    "# pickle.dump(loader, 'loader.pickle')\n",
    "\n",
    "with open('production_model.pickle', 'rb') as f:\n",
    "    loaded_production_model = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_querys = ['', 'bad', 'good', 'too long '*int(max_len)]\n",
    "for query in test_querys:\n",
    "    print(loaded_production_model.predict(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_production_model.predict(test_querys)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
