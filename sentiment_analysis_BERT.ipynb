{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "sentiment_analysis_BERT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2imkixTjwzwK"
      },
      "source": [
        "This notebook contains the code to use BERT with an extra layer of nodes added at the output to predict wether a movie review is positve or negative. Hopefully it will show the improved performance due to transfer learning at the cost of increased training time and model size compared to the models in the other notebook in this repo.\n",
        "\n",
        "\n",
        "Note the majority of this code is taken from: https://medium.com/tensorflow-2-bert-movie-review-sentiment-analysis/tensorflow-2-bert-movie-review-sentiment-analysis-b4ccabb87824\n",
        "\n",
        "but I hope to demonstrate some of the pros and cons of using BERT compared to simpler models and vectorisation techniques.\n",
        "I have also commented the code myself to demonstrate understanding of what is required to fine tune BERT to a specific task.\n",
        "\n",
        "\n",
        "## MAKE SURE YOU ENABLE GPU FOR GOOGLE COLAB!!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1z48se9wzwR",
        "outputId": "6e08acba-3720-4d40-d4a0-8f45bdd23872"
      },
      "source": [
        "!pip install bert-for-tf2"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bert-for-tf2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/d3/820ccaf55f1e24b5dd43583ac0da6d86c2d27bbdfffadbba69bafe73ca93/bert-for-tf2-0.14.7.tar.gz (41kB)\n",
            "\r\u001b[K     |████████                        | 10kB 27.5MB/s eta 0:00:01\r\u001b[K     |████████████████                | 20kB 12.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 30kB 13.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 40kB 13.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 6.5MB/s \n",
            "\u001b[?25hCollecting py-params>=0.9.6\n",
            "  Downloading https://files.pythonhosted.org/packages/75/2c/2256f28ef35946682ce703e69de914773c3f62048f4de6966d4e2dc1930a/py-params-0.10.1.tar.gz\n",
            "Collecting params-flow>=0.8.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a9/95/ff49f5ebd501f142a6f0aaf42bcfd1c192dc54909d1d9eb84ab031d46056/params-flow-0.8.2.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.41.1)\n",
            "Building wheels for collected packages: bert-for-tf2, py-params, params-flow\n",
            "  Building wheel for bert-for-tf2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.14.7-cp36-none-any.whl size=30537 sha256=54331ab6aae7afd94b27f65326abd166d3220431e75b01732ebe6afe43974822\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/f8/e2/b98f79a6b8cc898d8e4102b83acb8a098df7d27500a2bac912\n",
            "  Building wheel for py-params (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-params: filename=py_params-0.10.1-cp36-none-any.whl size=7849 sha256=08342733b61d6b2587e75d4b4da1f88438be033f28706a1723cb4af8598bbb45\n",
            "  Stored in directory: /root/.cache/pip/wheels/fd/13/cf/731530f5760266e69a40217ea27fa0d39a2d2a67230a73e2bc\n",
            "  Building wheel for params-flow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for params-flow: filename=params_flow-0.8.2-cp36-none-any.whl size=19474 sha256=f7fb48e7a07df26e08ca7f9c27e5ccb6468a2b6e45b7e7392c8345193ac30551\n",
            "  Stored in directory: /root/.cache/pip/wheels/08/c8/7f/81c86b9ff2b86e2c477e3914175be03e679e596067dc630c06\n",
            "Successfully built bert-for-tf2 py-params params-flow\n",
            "Installing collected packages: py-params, params-flow, bert-for-tf2\n",
            "Successfully installed bert-for-tf2-0.14.7 params-flow-0.8.2 py-params-0.10.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdwLOw9uwzwS",
        "outputId": "d86e4555-cee5-4aac-f7fa-14001afe94fa"
      },
      "source": [
        "\r\n",
        "# Import modules\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import bert\r\n",
        "import tensorflow as tf\r\n",
        "import tensorflow_hub as hub\r\n",
        "from tensorflow.keras.utils import to_categorical\r\n",
        "from tensorflow.keras.models import  Model\r\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout\r\n",
        "from tensorflow.keras.optimizers import Adam\r\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\r\n",
        "from tqdm import tqdm\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from tensorflow.keras.utils import to_categorical\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "from tensorflow import keras\r\n",
        "import os\r\n",
        "import re\r\n",
        "\r\n",
        "print(\"TensorFlow Version:\",tf.__version__)\r\n",
        "print(\"Hub version: \",hub.__version__)\r\n",
        "pd.set_option('display.max_colwidth',1000)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow Version: 2.4.0\n",
            "Hub version:  0.11.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgjstFv3wzwT"
      },
      "source": [
        "## Load Data\n",
        "Load dataset using Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eN95Do6xuMl"
      },
      "source": [
        "\r\n",
        "# Load all files from a directory in a DataFrame.\r\n",
        "def load_directory_data(directory):\r\n",
        "  data = {}\r\n",
        "  data[\"sentence\"] = []\r\n",
        "  data[\"sentiment\"] = []\r\n",
        "  for file_path in os.listdir(directory):\r\n",
        "    with tf.io.gfile.GFile(os.path.join(directory, file_path), \"r\") as f:\r\n",
        "      data[\"sentence\"].append(f.read())\r\n",
        "      data[\"sentiment\"].append(re.match(\"\\d+_(\\d+)\\.txt\", file_path).group(1))\r\n",
        "  return pd.DataFrame.from_dict(data)\r\n",
        "\r\n",
        "# Merge positive and negative examples, add a polarity column and shuffle.\r\n",
        "def load_dataset(directory):\r\n",
        "  pos_df = load_directory_data(os.path.join(directory, \"pos\"))\r\n",
        "  neg_df = load_directory_data(os.path.join(directory, \"neg\"))\r\n",
        "  pos_df[\"polarity\"] = 1\r\n",
        "  neg_df[\"polarity\"] = 0\r\n",
        "  # return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)\r\n",
        "  return pd.concat([pos_df, neg_df])\r\n",
        "\r\n",
        "# Download and process the dataset files.\r\n",
        "def download_and_load_datasets(force_download=False):\r\n",
        "  dataset = tf.keras.utils.get_file(\r\n",
        "      fname=\"aclImdb.tar.gz\", \r\n",
        "      origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", \r\n",
        "      extract=True)\r\n",
        "  \r\n",
        "  train_df = load_dataset(os.path.join(os.path.dirname(dataset), \r\n",
        "                                       \"aclImdb\", \"train\"))\r\n",
        "  test_df = load_dataset(os.path.join(os.path.dirname(dataset), \r\n",
        "                                      \"aclImdb\", \"test\"))\r\n",
        "  \r\n",
        "  return train_df.drop(columns = ['sentiment']), test_df.drop( columns = ['sentiment'])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AeEsSE2Sx7Pq",
        "outputId": "7bce863e-aad9-4a39-fc67-dc65486b053b"
      },
      "source": [
        "train, test = download_and_load_datasets()\r\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "84131840/84125825 [==============================] - 4s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "RmOOGmfrzBUH",
        "outputId": "18db0fc7-7e1c-4d34-b07f-8fc5cf34f1c8"
      },
      "source": [
        "train.head(2)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>polarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Director Brian Yuzna has had an uneven career in the horror genre, creating masterpieces such as \"Return of the Living Dead 3\" or \"Bride of Re-Animator\", but at the same time he has done awful movies such as \"Faust: Love for the Damned\" or the mediocre \"Progeny\". He is obviously better in the seat of Producer where his work producing Stuart Gordon's films has been superb.&lt;br /&gt;&lt;br /&gt;\"The Dentist\", is one of his lesser works as director, but the low profile it has benefits the film and its lack of pretensions makes it a very enjoyable experience. It tells the story of Dr. Alan Feinstone (played superbly by Corbin Bernsen), a successful dentist who one day discovers that his perfect life is not really as perfect as he thought when he discovers that his beautiful wife (Linda Hoffman)has an affair with the pool boy. This event disturbs his mind and puts him in a killing spree as he takes revenge on the world for being so \"filthy\".&lt;br /&gt;&lt;br /&gt;The premise is very well handled by Yuzna, a...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The scintillating Elizabeth Taylor stars in this lesser-known classic as a young girl from London who falls in love with a tea plantation owner from British Ceylon (current day Sri Lanka). Upon arrival she instantly feels out of place and is forced to adapt to the new culture as well as be in constant awareness of the angry elephant herd. William Dieterle, who also directed The Life Of Emile Zola and Portrait Of Jennie , does a masterful job of bringing a somewhat dark, and almost eerie, undertone to this romance and the setting is one of the most beautiful I've seen with the black and white themed mansion and the gorgeous island scenery.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  sentence  polarity\n",
              "0  Director Brian Yuzna has had an uneven career in the horror genre, creating masterpieces such as \"Return of the Living Dead 3\" or \"Bride of Re-Animator\", but at the same time he has done awful movies such as \"Faust: Love for the Damned\" or the mediocre \"Progeny\". He is obviously better in the seat of Producer where his work producing Stuart Gordon's films has been superb.<br /><br />\"The Dentist\", is one of his lesser works as director, but the low profile it has benefits the film and its lack of pretensions makes it a very enjoyable experience. It tells the story of Dr. Alan Feinstone (played superbly by Corbin Bernsen), a successful dentist who one day discovers that his perfect life is not really as perfect as he thought when he discovers that his beautiful wife (Linda Hoffman)has an affair with the pool boy. This event disturbs his mind and puts him in a killing spree as he takes revenge on the world for being so \"filthy\".<br /><br />The premise is very well handled by Yuzna, a...         1\n",
              "1                                                                                                                                                                                                                                                                                                                                                                   The scintillating Elizabeth Taylor stars in this lesser-known classic as a young girl from London who falls in love with a tea plantation owner from British Ceylon (current day Sri Lanka). Upon arrival she instantly feels out of place and is forced to adapt to the new culture as well as be in constant awareness of the angry elephant herd. William Dieterle, who also directed The Life Of Emile Zola and Portrait Of Jennie , does a masterful job of bringing a somewhat dark, and almost eerie, undertone to this romance and the setting is one of the most beautiful I've seen with the black and white themed mansion and the gorgeous island scenery.         1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llN0J44_wzwV"
      },
      "source": [
        "## Preprocessing\n",
        "BERT creates embedding from 3 inputs: token, segment & position embeddings. Here we will create functions to create these inputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lR_UT6IdwzwW"
      },
      "source": [
        "# Functions for constructing BERT Embeddings: input_ids, input_masks, input_segments and Inputs\n",
        "\n",
        "## this function creates the mask embeddings where simply 1 for real tokens and 0 for embeddings\n",
        "MAX_SEQ_LEN=500 # max sequence length\n",
        "def get_masks(tokens):\n",
        "    \"\"\"Masks: 1 for real tokens and 0 for paddings\"\"\"\n",
        "    return [1]*len(tokens) + [0] * (MAX_SEQ_LEN - len(tokens))\n",
        " "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qos2KqbBwzwW"
      },
      "source": [
        "\"\"\" this function creates the segment embeddings i.e. BERT is trained on 2 sentences to predict masked words and\n",
        "    the next sentence therefore the input shpuld be 2 sentences. With 0 for the first and 1 for the second\"\"\"\n",
        "\n",
        "def get_segments(tokens):\n",
        "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"  \n",
        "    segments = []\n",
        "    current_segment_id = 0\n",
        "    for token in tokens:\n",
        "        segments.append(current_segment_id)\n",
        "        if token == \"[SEP]\":\n",
        "            current_segment_id = 1\n",
        "    return segments + [0] * (MAX_SEQ_LEN - len(tokens))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5y5KJv0YwzwW"
      },
      "source": [
        "## gets token ids from BERT's vocabulary\n",
        "def get_ids(tokens, tokenizer):\n",
        "    \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
        "    token_ids = tokenizer.convert_tokens_to_ids(tokens,)\n",
        "    input_ids = token_ids + [0] * (MAX_SEQ_LEN - len(token_ids))\n",
        "    return input_ids"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHWhf5ZdwzwX"
      },
      "source": [
        "## tokenize the input, cut it to the max length, and then create input, mask and segment embeddings\n",
        "def create_single_input(sentence, tokenizer, max_len):\n",
        "    \"\"\"Create an input from a sentence\"\"\"\n",
        "    stokens = tokenizer.tokenize(sentence)\n",
        "    stokens = stokens[:max_len]\n",
        "    stokens = [\"[CLS]\"] + stokens + [\"[SEP]\"]\n",
        " \n",
        "    ids = get_ids(stokens, tokenizer)\n",
        "    masks = get_masks(stokens)\n",
        "    segments = get_segments(stokens)\n",
        "    \n",
        "    return ids, masks, segments"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsQzbw2vwzwX"
      },
      "source": [
        " ## create features out of whole movie review, NOT JUST FIRST 2 SENTENCES!!\n",
        "def convert_sentences_to_features(sentences, tokenizer):\n",
        "    \"\"\"Convert sentences to features: input_ids, input_masks and input_segments\"\"\"\n",
        "    input_ids, input_masks, input_segments = [], [], []\n",
        " \n",
        "    for sentence in tqdm(sentences,position=0, leave=True):\n",
        "        ids,masks,segments=create_single_input(sentence,tokenizer,MAX_SEQ_LEN-2)\n",
        "        assert len(ids) == MAX_SEQ_LEN\n",
        "        assert len(masks) == MAX_SEQ_LEN\n",
        "        assert len(segments) == MAX_SEQ_LEN\n",
        "        input_ids.append(ids)\n",
        "        input_masks.append(masks)\n",
        "        input_segments.append(segments)\n",
        "\n",
        "    return [np.asarray(input_ids, dtype=np.int32), \n",
        "          np.asarray(input_masks, dtype=np.int32), \n",
        "          np.asarray(input_segments, dtype=np.int32)]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZiVd2tbDwzwX"
      },
      "source": [
        "## use bert tokenizer by loading bert vocabualry and tokenizer\n",
        "def create_tonkenizer(bert_layer):\n",
        "    \"\"\"Instantiate Tokenizer with vocab\"\"\"\n",
        "    vocab_file=bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "    do_lower_case=bert_layer.resolved_object.do_lower_case.numpy() \n",
        "    tokenizer=bert.bert_tokenization.FullTokenizer(vocab_file,do_lower_case)\n",
        "    return tokenizer"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDDQkEc22_i4"
      },
      "source": [
        "## create instance of bert model\r\n",
        "- add 768 nodes with relu's and 2 output nodes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FTddDFnwzwY",
        "outputId": "72a45c0d-c672-482c-9e85-f7e42b8afbea"
      },
      "source": [
        "def nlp_model(callable_object):\r\n",
        "    # Load the pre-trained BERT base model\r\n",
        "    bert_layer = hub.KerasLayer(handle=callable_object, trainable=True)  \r\n",
        "   \r\n",
        "    # BERT layer three inputs: ids, masks and segments\r\n",
        "    input_ids = Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32, name=\"input_ids\")           \r\n",
        "    input_masks = Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32, name=\"input_masks\")       \r\n",
        "    input_segments = Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32, name=\"segment_ids\")\r\n",
        "    \r\n",
        "    inputs = [input_ids, input_masks, input_segments] # BERT inputs\r\n",
        "    pooled_output, sequence_output = bert_layer(inputs) # BERT outputs\r\n",
        "    \r\n",
        "    # Add a hidden layer\r\n",
        "    x = Dense(units=768, activation='relu')(pooled_output)\r\n",
        "    x = Dropout(0.1)(x)\r\n",
        " \r\n",
        "    # Add output layer\r\n",
        "    outputs = Dense(2, activation=\"softmax\")(x)\r\n",
        "\r\n",
        "    # Construct a new model\r\n",
        "    model = Model(inputs=inputs, outputs=outputs)\r\n",
        "    return model\r\n",
        "\r\n",
        "model = nlp_model(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\")\r\n",
        "model.summary()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_ids (InputLayer)          [(None, 500)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_masks (InputLayer)        [(None, 500)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "segment_ids (InputLayer)        [(None, 500)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "keras_layer (KerasLayer)        [(None, 768), (None, 109482241   input_ids[0][0]                  \n",
            "                                                                 input_masks[0][0]                \n",
            "                                                                 segment_ids[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 768)          590592      keras_layer[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 768)          0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 2)            1538        dropout[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 110,074,371\n",
            "Trainable params: 110,074,370\n",
            "Non-trainable params: 1\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kl2hkoX4A1h"
      },
      "source": [
        "Lets check we are creating features correctly for reviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kp4BV7q3ww0",
        "outputId": "d347fa0d-49f1-4526-a10e-113c4b7acd88"
      },
      "source": [
        "review = train['sentence'].head(1)\r\n",
        "tokenizer = create_tonkenizer(model.layers[3])\r\n",
        "features = convert_sentences_to_features(review, tokenizer)\r\n",
        "                    \r\n",
        "print(review)\r\n",
        "print('token_ids : ',features[0])\r\n",
        "print('mask embeddings : ', features[1])\r\n",
        "print('segment ids : ', features[2])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 141.04it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0    Director Brian Yuzna has had an uneven career in the horror genre, creating masterpieces such as \"Return of the Living Dead 3\" or \"Bride of Re-Animator\", but at the same time he has done awful movies such as \"Faust: Love for the Damned\" or the mediocre \"Progeny\". He is obviously better in the seat of Producer where his work producing Stuart Gordon's films has been superb.<br /><br />\"The Dentist\", is one of his lesser works as director, but the low profile it has benefits the film and its lack of pretensions makes it a very enjoyable experience. It tells the story of Dr. Alan Feinstone (played superbly by Corbin Bernsen), a successful dentist who one day discovers that his perfect life is not really as perfect as he thought when he discovers that his beautiful wife (Linda Hoffman)has an affair with the pool boy. This event disturbs his mind and puts him in a killing spree as he takes revenge on the world for being so \"filthy\".<br /><br />The premise is very well handled by Yuzna, a...\n",
            "Name: sentence, dtype: object\n",
            "token_ids :  [[  101  2472  4422  9805  2480  2532  2038  2018  2019 17837  2476  1999\n",
            "   1996  5469  6907  1010  4526 17743  2015  2107  2004  1000  2709  1997\n",
            "   1996  2542  2757  1017  1000  2030  1000  8959  1997  2128  1011 25132\n",
            "   1000  1010  2021  2012  1996  2168  2051  2002  2038  2589  9643  5691\n",
            "   2107  2004  1000 24021  1024  2293  2005  1996  9636  1000  2030  1996\n",
            "  19960  3695 16748  1000  4013 17487  1000  1012  2002  2003  5525  2488\n",
            "   1999  1996  2835  1997  3135  2073  2010  2147  5155  6990  5146  1005\n",
            "   1055  3152  2038  2042 21688  1012  1026  7987  1013  1028  1026  7987\n",
            "   1013  1028  1000  1996 24385  1000  1010  2003  2028  1997  2010  8276\n",
            "   2573  2004  2472  1010  2021  1996  2659  6337  2009  2038  6666  1996\n",
            "   2143  1998  2049  3768  1997  3653 29048  2015  3084  2009  1037  2200\n",
            "  22249  3325  1012  2009  4136  1996  2466  1997  2852  1012  5070 27132\n",
            "   9221  1006  2209 21688  2135  2011 24003 16595  5054  1007  1010  1037\n",
            "   3144 24385  2040  2028  2154  9418  2008  2010  3819  2166  2003  2025\n",
            "   2428  2004  3819  2004  2002  2245  2043  2002  9418  2008  2010  3376\n",
            "   2564  1006  8507 15107  1007  2038  2019  6771  2007  1996  4770  2879\n",
            "   1012  2023  2724 22995  2015  2010  2568  1998  8509  2032  1999  1037\n",
            "   4288 11867  9910  2004  2002  3138  7195  2006  1996  2088  2005  2108\n",
            "   2061  1000 18294  1000  1012  1026  7987  1013  1028  1026  7987  1013\n",
            "   1028  1996 18458  2003  2200  2092  8971  2011  9805  2480  2532  1010\n",
            "   2004  2002  3138  2149  2006  1037  4536  2206 27132  9221  1005  1055\n",
            "   2154  1997  7195  1012  2054  3084  2023  3185  2367  2013  2087 18296\n",
            "   2545  2003  2008  2057  2024  2025  1999  1996  6778  1005  1055  7339\n",
            "   1010  2057  3582 27132  9221  2138  2002  2003  1996  2364  2839  1012\n",
            "   2057  7409  2129  2002  3632  2013  9768  2658  2000 18224  4028  1999\n",
            "   1037  2154  1012  9805  2480  2532  9020  2000  2507  1996  3185  1996\n",
            "   6635  3815  1997 23873  2021  9909  1037  2204 13004  1997  2601  8562\n",
            "   2008  2428  7126  1996  3185  1012  1026  7987  1013  1028  1026  7987\n",
            "   1013  1028  2087  1997  1996  3112  1997  1996 18458  2003  1999 16595\n",
            "   5054  1005  1055  2836  2004 27132  9221  1012  2002  2064  2191  2017\n",
            "   2514 11883  1998  5223  2875  2032  2012  1996  2168  2051  1010  1998\n",
            "   1996 11259  8562  2010  2839  2038  2003  2178  7814  2008  8387  1996\n",
            "   2143  1012  1996  2717  1997  1996  3459  2003  2025  2004  2204  1010\n",
            "   1998  1045  2228  2008  2037  4942 11968  3772 13403  1996  2143  2062\n",
            "   2084  2009  2323  1012  1037  3862  6453  2003  6358 18921  2063  1010\n",
            "   2004  1996  6317  2667  2000  4608 27132  9221  1012  2096  2010  2112\n",
            "   2003  3243  2235  1010  2002  3084  1037  2307  3105  2007  2009  1012\n",
            "   1026  7987  1013  1028  1026  7987  1013  1028  2007  1037 24385  2004\n",
            "   6359  1010  2175  2854  5019  2024  3517  1010  1998  4422  9805  2480\n",
            "   2532 18058  2307 16420  2595  1999  1996  6149  3815  1012  2009  1005\n",
            "   1055  2204  2000  2156  2008  2002  2515  2025  2175  2058  1011  1996\n",
            "   1011  2327  2007  2009  2004  2002  2788   102]]\n",
            "mask embeddings :  [[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]\n",
            "segment ids :  [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJYi5_MZ9Z5i"
      },
      "source": [
        "## Model Training\r\n",
        " Looks fine lets train the model\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NNTTXRY9hgq",
        "outputId": "9f7e6448-4fe9-4b36-d9d0-5b38613909ba"
      },
      "source": [
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "train = train.sample(frac=1) # Shuffle the dataset\r\n",
        "train_frac = int(0.75*train.shape[0])\r\n",
        "train_df = train[:train_frac]\r\n",
        "val_df = train[train_frac:]\r\n",
        "\r\n",
        "tokenizer = create_tonkenizer(model.layers[3])\r\n",
        "X_train = convert_sentences_to_features(train_df['sentence'], tokenizer)\r\n",
        "X_val = convert_sentences_to_features(val_df['sentence'], tokenizer)\r\n",
        "X_test = convert_sentences_to_features(test['sentence'], tokenizer)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "y_train = to_categorical(train_df['polarity'].values)\r\n",
        "y_val = to_categorical(val_df['polarity'].values)\r\n",
        "y_test = to_categorical(test['polarity'].values)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "print(len(y_train))\r\n",
        "print(len(y_val))\r\n",
        "print(len(y_test))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 18750/18750 [01:01<00:00, 304.03it/s]\n",
            "100%|██████████| 6250/6250 [00:20<00:00, 307.58it/s]\n",
            "100%|██████████| 25000/25000 [01:20<00:00, 312.25it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "18750\n",
            "6250\n",
            "25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRLxQljq_DWk",
        "outputId": "8c52ea55-6e2c-4d22-a17d-f2798e27173d"
      },
      "source": [
        "# Train the model\r\n",
        "BATCH_SIZE = 8\r\n",
        "EPOCHS = 2\r\n",
        "\r\n",
        "# Use Adam optimizer to minimize the categorical_crossentropy loss\r\n",
        "opt = Adam(learning_rate=2e-5)\r\n",
        "model.compile(optimizer=opt, \r\n",
        "              loss='categorical_crossentropy', \r\n",
        "              metrics=['accuracy'])\r\n",
        "\r\n",
        "# Fit the data to the model\r\n",
        "history = model.fit(X_train, y_train,\r\n",
        "                    validation_data=(X_val, y_val),\r\n",
        "                    epochs=EPOCHS,\r\n",
        "                    batch_size=BATCH_SIZE,\r\n",
        "                    verbose = 1)\r\n",
        "\r\n",
        "# Save the trained model\r\n",
        "model.save('nlp_model.h5')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "2344/2344 [==============================] - 2557s 1s/step - loss: 0.3266 - accuracy: 0.8556 - val_loss: 0.2245 - val_accuracy: 0.9136\n",
            "Epoch 2/2\n",
            "2344/2344 [==============================] - 2542s 1s/step - loss: 0.0984 - accuracy: 0.9676 - val_loss: 0.2018 - val_accuracy: 0.9221\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcj0qmLVTV4D"
      },
      "source": [
        "## Evaluate Model Performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7bZsZfFC7Xn"
      },
      "source": [
        "\r\n",
        "# Load the pretrained nlp_model\r\n",
        "from tensorflow.keras.models import load_model\r\n",
        "new_model = load_model('nlp_model.h5',custom_objects={'KerasLayer':hub.KerasLayer})"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIg0go-ES5dg"
      },
      "source": [
        "# Predict on test dataset\r\n",
        "from sklearn.metrics import classification_report\r\n",
        "pred_test = np.argmax(new_model.predict(X_test), axis=1)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLOdI-uATKrX",
        "outputId": "1cfa0e9a-1df5-4bdc-962b-5746dc102168"
      },
      "source": [
        "print(classification_report(np.argmax(y_test,axis=1), pred_test))\r\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.89      0.92     12500\n",
            "           1       0.89      0.96      0.93     12500\n",
            "\n",
            "    accuracy                           0.92     25000\n",
            "   macro avg       0.93      0.92      0.92     25000\n",
            "weighted avg       0.93      0.92      0.92     25000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xo9ngl9bTTLa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}